{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba79f2-44db-48df-8cac-7843613b2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# # Scan all CSV files\n",
    "# cleaned_path = Path(\"meta_scan_csvs/cleaned\")\n",
    "# csv_files = list(cleaned_path.rglob(\"*.csv\"))\n",
    "\n",
    "# # Create metadata inventory\n",
    "# for csv_file in csv_files_clean:\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     print(f\"  Loaded clean {csv_file.name}: {df.shape}\")\n",
    "\n",
    "# revealed files with the least features to remove (so we can use more features for our model)\n",
    "    # lab_person_4_cleaned.csv: 32 columns\n",
    "    # kitchen_7_cleaned.csv: 34 columns\n",
    "    # kitchen_8_cleaned.csv: 35 columns\n",
    "    # kitchen_person_1_cleaned.csv: 35 columns\n",
    "    # kitchen_5_cleaned.csv: 38 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a8839-263b-4bad-862e-354d4c58357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def parse_filename(filepath):\n",
    "    \"\"\"\n",
    "    Parse filename to extract room_type, scan_type, and trial_number\n",
    "    Examples:\n",
    "    - blinds_1_cleaned.csv -> room: blinds, scan: base, trial: 1\n",
    "    - blinds_up_1_cleaned.csv -> room: blinds_up, scan: base, trial: 1\n",
    "    - kitchen_motion_3_cleaned.csv -> room: kitchen, scan: motion, trial: 3\n",
    "    \"\"\"\n",
    "    filename = filepath.stem.replace('_cleaned', '')\n",
    "    \n",
    "    # Handle blinds_up as a special case (two-word room type)\n",
    "    if filename.startswith('blinds_up'):\n",
    "        room_type = 'blinds_up'\n",
    "        remainder = filename.replace('blinds_up_', '', 1)\n",
    "    else:\n",
    "        # Split and take first part as room type\n",
    "        parts = filename.split('_')\n",
    "        room_type = parts[0]\n",
    "        remainder = '_'.join(parts[1:])\n",
    "    \n",
    "    # Now parse the remainder for scan type and trial\n",
    "    remainder_parts = remainder.split('_')\n",
    "    \n",
    "    if len(remainder_parts) == 1:\n",
    "        # Base scan (just a number)\n",
    "        scan_type = 'base'\n",
    "        trial_number = int(remainder_parts[0])\n",
    "    elif len(remainder_parts) == 2:\n",
    "        # Scan type + number (e.g., motion_3)\n",
    "        scan_type = remainder_parts[0]\n",
    "        trial_number = int(remainder_parts[1])\n",
    "    else:\n",
    "        scan_type = 'unknown'\n",
    "        trial_number = -1\n",
    "    \n",
    "    return room_type, scan_type, trial_number\n",
    "\n",
    "# Scan all CSV files\n",
    "cleaned_path = Path(\"meta_scan_csvs/cleaned\")\n",
    "csv_files = list(cleaned_path.rglob(\"*.csv\"))\n",
    "\n",
    "# Create metadata inventory\n",
    "metadata = []\n",
    "for csv_file in csv_files:\n",
    "    room_type, scan_type, trial_number = parse_filename(csv_file)\n",
    "    \n",
    "    # Get number of rows\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"  Loaded {csv_file.name}: {df.shape}\")\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    metadata.append({\n",
    "        'filepath': str(csv_file),\n",
    "        'filename': csv_file.name,\n",
    "        'room_type': room_type,\n",
    "        'scan_type': scan_type,\n",
    "        'trial_number': trial_number,\n",
    "        'num_rows': num_rows,\n",
    "        'folder': csv_file.parent.name\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "# Display summary\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA INVENTORY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal CSV files: {len(metadata_df)}\")\n",
    "print(f\"\\nRoom types ({len(metadata_df['room_type'].unique())}): {sorted(metadata_df['room_type'].unique())}\")\n",
    "print(f\"Scan types ({len(metadata_df['scan_type'].unique())}): {sorted(metadata_df['scan_type'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COUNTS BY ROOM TYPE AND SCAN TYPE\")\n",
    "print(\"=\" * 80)\n",
    "pivot = metadata_df.groupby(['room_type', 'scan_type']).size().unstack(fill_value=0)\n",
    "print(pivot)\n",
    "print(f\"\\nTotal scans per room type:\")\n",
    "print(pivot.sum(axis=1))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROW COUNTS STATISTICS BY ROOM TYPE\")\n",
    "print(\"=\" * 80)\n",
    "print(metadata_df.groupby('room_type')['num_rows'].describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE FILES (sorted by room_type, scan_type, trial)\")\n",
    "print(\"=\" * 80)\n",
    "sample = metadata_df.sort_values(['room_type', 'scan_type', 'trial_number'])\n",
    "print(sample[['filename', 'room_type', 'scan_type', 'trial_number', 'num_rows']].head(15))\n",
    "\n",
    "# Verify we have exactly 105 scans\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Expected: 100 scans (was 105, removed 5 with insufficient features)\")\n",
    "print(f\"Actual: {len(metadata_df)} scans\")\n",
    "if len(metadata_df) == 100:\n",
    "    print(\"✓ Count matches!\")\n",
    "else:\n",
    "    print(\"⚠ Count mismatch - please review\")\n",
    "\n",
    "# Save metadata to CSV for reference\n",
    "metadata_df.to_csv(\"meta_scan_csvs/data_inventory.csv\", index=False)\n",
    "print(f\"\\n✓ Saved complete inventory to: meta_scan_csvs/data_inventory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b2fc0-ec3c-4ae4-9fbb-49cf98095ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load the inventory we just created\n",
    "metadata_df = pd.read_csv(\"meta_scan_csvs/data_inventory.csv\")\n",
    "\n",
    "# Create a stratification column combining room_type and scan_type\n",
    "metadata_df['strata'] = metadata_df['room_type'] + '_' + metadata_df['scan_type']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MANUAL STRATIFIED TRAIN/TEST SPLIT (~80/20)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Manually assign splits to ensure good distribution\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for strata in metadata_df['strata'].unique():\n",
    "    strata_df = metadata_df[metadata_df['strata'] == strata]\n",
    "    n = len(strata_df)\n",
    "    \n",
    "    # Calculate split\n",
    "    n_test = max(1, round(n * 0.2))  # At least 1 for test\n",
    "    n_train = n - n_test\n",
    "    \n",
    "    print(f\"{strata}: {n} total → {n_train} train, {n_test} test\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    shuffled = strata_df.sample(frac=1, random_state=42)\n",
    "    test_indices.extend(shuffled.index[:n_test].tolist())\n",
    "    train_indices.extend(shuffled.index[n_test:].tolist())\n",
    "\n",
    "train_df = metadata_df.loc[train_indices]\n",
    "test_df = metadata_df.loc[test_indices]\n",
    "\n",
    "print(f\"\\nTotal files: {len(metadata_df)}\")\n",
    "print(f\"Training files: {len(train_df)} ({len(train_df)/len(metadata_df)*100:.1f}%)\")\n",
    "print(f\"Testing files: {len(test_df)} ({len(test_df)/len(metadata_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SET DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "train_dist = train_df.groupby(['room_type', 'scan_type']).size().unstack(fill_value=0)\n",
    "print(train_dist)\n",
    "print(f\"\\nTotal per room type:\\n{train_dist.sum(axis=1)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING SET DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "test_dist = test_df.groupby(['room_type', 'scan_type']).size().unstack(fill_value=0)\n",
    "print(test_dist)\n",
    "print(f\"\\nTotal per room type:\\n{test_dist.sum(axis=1)}\")\n",
    "\n",
    "# Save the splits\n",
    "train_df.to_csv(\"meta_scan_csvs/train_metadata.csv\", index=False)\n",
    "test_df.to_csv(\"meta_scan_csvs/test_metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ meta_scan_csvs/train_metadata.csv\")\n",
    "print(\"✓ meta_scan_csvs/test_metadata.csv\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE TRAINING FILES\")\n",
    "print(\"=\" * 80)\n",
    "print(train_df[['filename', 'room_type', 'scan_type', 'trial_number']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE TESTING FILES\")\n",
    "print(\"=\" * 80)\n",
    "print(test_df[['filename', 'room_type', 'scan_type', 'trial_number']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca7845-0aed-4768-976c-3be7bfa2b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sktime --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331af0c-9b67-4237-a376-289c2e4fe092",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_path = Path(\"meta_scan_csvs/cleaned\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING SENSOR COLUMNS ACROSS ALL 100 CSVs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_columns_sets = []\n",
    "\n",
    "for csv_file in cleaned_path.rglob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    cols = [c for c in df.columns if c != 'Time (s)']\n",
    "    all_columns_sets.append(set(cols))\n",
    "    print(f\"{csv_file.name}: {len(cols)} columns\")\n",
    "\n",
    "# Find common columns across ALL CSVs\n",
    "common_columns = set.intersection(*all_columns_sets)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"COMMON COLUMNS ACROSS ALL 100 CSVs: {len(common_columns)}\")\n",
    "print(\"=\" * 80)\n",
    "print(sorted(common_columns))\n",
    "\n",
    "# Find the range of column counts\n",
    "all_column_counts = [len(s) for s in all_columns_sets]\n",
    "print(f\"\\nColumn count range: {min(all_column_counts)} to {max(all_column_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af098a-7cdc-48d1-bf1d-8b91accd53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the common columns (from the analysis above)\n",
    "COMMON_COLUMNS = [\n",
    "    '% Prims Clipped', '% Prims Trivially Rejected', '% Stalled on System Memory', \n",
    "    '% Texture L2 Miss', '% Vertex Fetch Stall', 'ALU / Fragment', 'ALU / Vertex', \n",
    "    'Average Polygon Area', 'Average Vertices / Polygon', 'Avg Bytes / Fragment', \n",
    "    'Avg Bytes / Vertex', 'Avg Preemption Delay', 'Clocks / Second', 'EFU / Fragment', \n",
    "    'EFU / Vertex', 'Fragment ALU Instructions / Sec (Full)', \n",
    "    'Fragment ALU Instructions / Sec (Half)', 'Fragment EFU Instructions / Second', \n",
    "    'Fragment Instructions / Second', 'Fragments Shaded / Second', 'GPU % Bus Busy', \n",
    "    'GPU Frequency', 'L1 Texture Cache Miss Per Pixel', 'Pre-clipped Polygons/Second', \n",
    "    'Preemptions / second', 'Read Total (Bytes/sec)', 'Reused Vertices / Second', \n",
    "    'SP Memory Read (Bytes/Second)', 'Texture Memory Read BW (Bytes/Second)', \n",
    "    'Textures / Fragment', 'Textures / Vertex', 'Vertex Instructions / Second', \n",
    "    'Vertex Memory Read (Bytes/Second)', 'Vertices Shaded / Second', \n",
    "    'Write Total (Bytes/sec)', 'app_gpu_ms', 'app_rss_mb', 'app_uss_mb', 'app_vss_mb', \n",
    "    'application_layer_count', 'application_prediction_milliseconds', 'available_memory_mb', \n",
    "    'cpu_frequency_mhz', 'cpu_level', 'cpu_util_0', 'cpu_util_1', 'cpu_util_2', \n",
    "    'cpu_util_3', 'cpu_util_4', 'cpu_util_5', 'display_refresh_rate', \n",
    "    'gpu_frequency_mhz', 'gpu_level', 'gpu_util', 'mem_frequency_mhz', \n",
    "    'stale_frames_per_second', 'timewarp_gpu_ms'\n",
    "]\n",
    "\n",
    "def create_windows(df, window_size=75):\n",
    "    \"\"\"\n",
    "    Create non-overlapping tumbling windows from a time series dataframe.\n",
    "    Returns a list of window dataframes.\n",
    "    \"\"\"\n",
    "    num_windows = len(df) // window_size\n",
    "    windows = []\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * window_size\n",
    "        end_idx = start_idx + window_size\n",
    "        window = df.iloc[start_idx:end_idx].copy()\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "def process_dataset(metadata_df, output_dir, dataset_name):\n",
    "    \"\"\"\n",
    "    Process all CSVs in a dataset (train or test), create windows using ONLY common features.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_windows_data = []\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PROCESSING {dataset_name.upper()} SET (COMMON FEATURES ONLY)\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        csv_path = Path(row['filepath'])\n",
    "        room_type = row['room_type']\n",
    "        scan_type = row['scan_type']\n",
    "        trial_number = row['trial_number']\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Select ONLY common columns (no Time column)\n",
    "        # Check which common columns exist in this CSV\n",
    "        available_common = [col for col in COMMON_COLUMNS if col in df.columns]\n",
    "        \n",
    "        if len(available_common) != len(COMMON_COLUMNS):\n",
    "            missing = set(COMMON_COLUMNS) - set(available_common)\n",
    "            print(f\"WARNING: {row['filename']} missing columns: {missing}\")\n",
    "        \n",
    "        df = df[available_common]\n",
    "        \n",
    "        # Create windows\n",
    "        windows = create_windows(df, window_size=75)\n",
    "        \n",
    "        print(f\"{row['filename']}: {len(df)} rows → {len(windows)} windows (57 features)\")\n",
    "        \n",
    "        # Save each window with metadata\n",
    "        for window_idx, window_df in enumerate(windows):\n",
    "            window_data = {\n",
    "                'original_filename': row['filename'],\n",
    "                'room_type': room_type,\n",
    "                'scan_type': scan_type,\n",
    "                'trial_number': trial_number,\n",
    "                'window_id': window_idx,\n",
    "                'window_data': window_df.values  # Store as numpy array\n",
    "            }\n",
    "            all_windows_data.append(window_data)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{dataset_name.upper()} SUMMARY\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Total CSVs processed: {len(metadata_df)}\")\n",
    "    print(f\"Total windows created: {len(all_windows_data)}\")\n",
    "    print(f\"Windows per CSV (avg): {len(all_windows_data) / len(metadata_df):.1f}\")\n",
    "    print(f\"Features per window: 57 (common features only)\")\n",
    "    \n",
    "    # Save as pickle for easy loading later\n",
    "    windows_df = pd.DataFrame(all_windows_data)\n",
    "    output_file = output_path / f\"{dataset_name}_windows_common.pkl\"\n",
    "    windows_df.to_pickle(output_file)\n",
    "    print(f\"\\n✓ Saved to: {output_file}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"\\nWindows by room type:\")\n",
    "    print(windows_df.groupby('room_type').size())\n",
    "    \n",
    "    return windows_df\n",
    "\n",
    "# Load train and test metadata\n",
    "train_metadata = pd.read_csv(\"meta_scan_csvs/train_metadata.csv\")\n",
    "test_metadata = pd.read_csv(\"meta_scan_csvs/test_metadata.csv\")\n",
    "\n",
    "# Process training set\n",
    "train_windows = process_dataset(\n",
    "    train_metadata, \n",
    "    \"meta_scan_csvs/windowed_data\", \n",
    "    \"train\"\n",
    ")\n",
    "\n",
    "# Process testing set\n",
    "test_windows = process_dataset(\n",
    "    test_metadata, \n",
    "    \"meta_scan_csvs/windowed_data\", \n",
    "    \"test\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"WINDOWING COMPLETE (COMMON FEATURES)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Training windows: {len(train_windows)}\")\n",
    "print(f\"Testing windows: {len(test_windows)}\")\n",
    "print(f\"Features per window: 57\")\n",
    "print(f\"No padding needed - all windows have same shape!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664b185-af33-46b8-b9a2-865e3f61edcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 4: ROCKET CLASSIFIER FOR ROOM TYPE (COMMON FEATURES)\n",
      "================================================================================\n",
      "\n",
      "Loading windowed data...\n",
      "Training windows: 34597\n",
      "Testing windows: 5932\n",
      "\n",
      "Preparing training data...\n",
      "X_train shape: (34597, 75, 57)\n",
      "  - Samples: 34597\n",
      "  - Time points: 75\n",
      "  - Features (common sensors): 57\n",
      "\n",
      "Preparing testing data...\n",
      "X_test shape: (5932, 75, 57)\n",
      "\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Training set:\n",
      "blinds        7368\n",
      "blinds_up     4726\n",
      "hallway      12907\n",
      "kitchen       4834\n",
      "lab           4762\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing set:\n",
      "blinds       1194\n",
      "blinds_up    1402\n",
      "hallway      1171\n",
      "kitchen      1030\n",
      "lab          1135\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "TRAINING ROCKET CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "Initializing ROCKET classifier...\n",
      "  - Number of kernels: 10,000\n",
      "  - Random state: 42\n",
      "\n",
      "Fitting ROCKET classifier (this may take a few minutes)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: ROCKET CLASSIFIER FOR ROOM TYPE (COMMON FEATURES)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load windowed data with common features\n",
    "print(\"\\nLoading windowed data...\")\n",
    "train_windows = pd.read_pickle(\"meta_scan_csvs/windowed_data/train_windows_common.pkl\")\n",
    "test_windows = pd.read_pickle(\"meta_scan_csvs/windowed_data/test_windows_common.pkl\")\n",
    "\n",
    "print(f\"Training windows: {len(train_windows)}\")\n",
    "print(f\"Testing windows: {len(test_windows)}\")\n",
    "\n",
    "# Prepare data for ROCKET (no padding needed!)\n",
    "def prepare_rocket_data(windows_df):\n",
    "    \"\"\"\n",
    "    Convert windowed data to ROCKET format: (n_samples, n_timepoints, n_features)\n",
    "    All windows have the same shape now!\n",
    "    \"\"\"\n",
    "    X = np.stack(windows_df['window_data'].values)  # Stack all windows\n",
    "    y = windows_df['room_type'].values  # Room type labels\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"\\nPreparing training data...\")\n",
    "X_train, y_train = prepare_rocket_data(train_windows)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"  - Samples: {X_train.shape[0]}\")\n",
    "print(f\"  - Time points: {X_train.shape[1]}\")\n",
    "print(f\"  - Features (common sensors): {X_train.shape[2]}\")\n",
    "\n",
    "print(\"\\nPreparing testing data...\")\n",
    "X_test, y_test = prepare_rocket_data(test_windows)\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTraining set:\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "print(train_dist)\n",
    "print(f\"\\nTesting set:\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "print(test_dist)\n",
    "\n",
    "# Train ROCKET classifier\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING ROCKET CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nInitializing ROCKET classifier...\")\n",
    "print(\"  - Number of kernels: 10,000\")\n",
    "print(\"  - Random state: 42\")\n",
    "\n",
    "rocket_classifier = RocketClassifier(num_kernels=10000, rocket_transform=\"minirocket\", random_state=42)\n",
    "\n",
    "print(\"\\nFitting ROCKET classifier (this may take a few minutes)...\")\n",
    "rocket_classifier.fit(X_train, y_train)\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMaking predictions on training set...\")\n",
    "y_train_pred = rocket_classifier.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Making predictions on test set...\")\n",
    "y_test_pred = rocket_classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (TEST SET)\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRIX (TEST SET)\")\n",
    "print(\"=\" * 80)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "room_types = sorted(np.unique(y_test))\n",
    "cm_df = pd.DataFrame(cm, index=room_types, columns=room_types)\n",
    "print(cm_df)\n",
    "print(\"\\nRows = Actual, Columns = Predicted\")\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "for i, room_type in enumerate(room_types):\n",
    "    class_accuracy = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"{room_type:12s}: {class_accuracy:.4f} ({class_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - NO MODEL SAVED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96951a6-6efc-4543-b8a3-417d6cddc83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b6816c-952d-49ac-b6c1-30dc267fcdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GRADIENT BOOSTING CLASSIFIER WITH FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROCESSING TRAIN SET\n",
      "================================================================================\n",
      "\n",
      "blinds_motion_5_cleaned.csv: 327 rows → 11 windows\n",
      "blinds_motion_3_cleaned.csv: 335 rows → 11 windows\n",
      "blinds_motion_1_cleaned.csv: 1838 rows → 71 windows\n",
      "blinds_motion_4_cleaned.csv: 312 rows → 10 windows\n",
      "blinds_object_5_cleaned.csv: 1827 rows → 71 windows\n",
      "blinds_object_3_cleaned.csv: 327 rows → 11 windows\n",
      "blinds_object_1_cleaned.csv: 1821 rows → 70 windows\n",
      "blinds_object_4_cleaned.csv: 325 rows → 11 windows\n",
      "blinds_person_5_cleaned.csv: 327 rows → 11 windows\n",
      "blinds_person_3_cleaned.csv: 326 rows → 11 windows\n",
      "blinds_person_1_cleaned.csv: 350 rows → 12 windows\n",
      "blinds_person_4_cleaned.csv: 312 rows → 10 windows\n",
      "hallway_5_cleaned.csv: 1493 rows → 57 windows\n",
      "hallway_3_cleaned.csv: 306 rows → 10 windows\n",
      "hallway_1_cleaned.csv: 300 rows → 10 windows\n",
      "hallway_4_cleaned.csv: 320 rows → 10 windows\n",
      "hallway_motion_5_cleaned.csv: 314 rows → 10 windows\n",
      "hallway_motion_3_cleaned.csv: 313 rows → 10 windows\n",
      "hallway_motion_1_cleaned.csv: 329 rows → 11 windows\n",
      "hallway_motion_4_cleaned.csv: 1869 rows → 72 windows\n",
      "hallway_object_5_cleaned.csv: 306 rows → 10 windows\n",
      "hallway_object_3_cleaned.csv: 1739 rows → 67 windows\n",
      "hallway_object_1_cleaned.csv: 2024 rows → 78 windows\n",
      "hallway_object_4_cleaned.csv: 304 rows → 10 windows\n",
      "hallway_person_5_cleaned.csv: 1799 rows → 69 windows\n",
      "hallway_person_3_cleaned.csv: 345 rows → 11 windows\n",
      "hallway_person_1_cleaned.csv: 340 rows → 11 windows\n",
      "hallway_person_4_cleaned.csv: 1977 rows → 77 windows\n",
      "kitchen_1_cleaned.csv: 320 rows → 10 windows\n",
      "kitchen_6_cleaned.csv: 250 rows → 8 windows\n",
      "kitchen_2_cleaned.csv: 296 rows → 9 windows\n",
      "kitchen_4_cleaned.csv: 1668 rows → 64 windows\n",
      "kitchen_3_cleaned.csv: 312 rows → 10 windows\n",
      "kitchen_9_cleaned.csv: 1971 rows → 76 windows\n",
      "kitchen_motion_5_cleaned.csv: 342 rows → 11 windows\n",
      "kitchen_motion_3_cleaned.csv: 1837 rows → 71 windows\n",
      "kitchen_motion_1_cleaned.csv: 313 rows → 10 windows\n",
      "kitchen_motion_4_cleaned.csv: 338 rows → 11 windows\n",
      "kitchen_object_5_cleaned.csv: 340 rows → 11 windows\n",
      "kitchen_object_3_cleaned.csv: 1839 rows → 71 windows\n",
      "kitchen_object_1_cleaned.csv: 1748 rows → 67 windows\n",
      "kitchen_object_4_cleaned.csv: 1839 rows → 71 windows\n",
      "kitchen_person_5_cleaned.csv: 322 rows → 10 windows\n",
      "kitchen_person_2_cleaned.csv: 335 rows → 11 windows\n",
      "kitchen_person_4_cleaned.csv: 308 rows → 10 windows\n",
      "lab_5_cleaned.csv: 1717 rows → 66 windows\n",
      "lab_3_cleaned.csv: 312 rows → 10 windows\n",
      "lab_1_cleaned.csv: 1749 rows → 67 windows\n",
      "lab_4_cleaned.csv: 312 rows → 10 windows\n",
      "lab_motion_5_cleaned.csv: 1859 rows → 72 windows\n",
      "lab_motion_3_cleaned.csv: 330 rows → 11 windows\n",
      "lab_motion_1_cleaned.csv: 300 rows → 10 windows\n",
      "lab_motion_4_cleaned.csv: 322 rows → 10 windows\n",
      "lab_object_5_cleaned.csv: 296 rows → 9 windows\n",
      "lab_object_3_cleaned.csv: 313 rows → 10 windows\n",
      "lab_object_1_cleaned.csv: 1850 rows → 72 windows\n",
      "lab_object_4_cleaned.csv: 297 rows → 9 windows\n",
      "lab_person_5_cleaned.csv: 309 rows → 10 windows\n",
      "lab_person_1_cleaned.csv: 341 rows → 11 windows\n",
      "lab_person_3_cleaned.csv: 328 rows → 11 windows\n",
      "\n",
      "TRAIN SUMMARY:\n",
      "Total windows: 1752\n",
      "Feature vector size: 310 (57 sensors × 5 stats)\n",
      "\n",
      "================================================================================\n",
      "PROCESSING TEST SET\n",
      "================================================================================\n",
      "\n",
      "blinds_motion_2_cleaned.csv: 329 rows → 11 windows\n",
      "blinds_object_2_cleaned.csv: 327 rows → 11 windows\n",
      "blinds_person_2_cleaned.csv: 313 rows → 10 windows\n",
      "hallway_2_cleaned.csv: 305 rows → 10 windows\n",
      "hallway_motion_2_cleaned.csv: 327 rows → 11 windows\n",
      "hallway_object_2_cleaned.csv: 1712 rows → 66 windows\n",
      "hallway_person_2_cleaned.csv: 1820 rows → 70 windows\n",
      "kitchen_10_cleaned.csv: 2064 rows → 80 windows\n",
      "kitchen_motion_2_cleaned.csv: 1647 rows → 63 windows\n",
      "kitchen_object_2_cleaned.csv: 326 rows → 11 windows\n",
      "kitchen_person_3_cleaned.csv: 1754 rows → 68 windows\n",
      "lab_2_cleaned.csv: 319 rows → 10 windows\n",
      "lab_motion_2_cleaned.csv: 317 rows → 10 windows\n",
      "lab_object_2_cleaned.csv: 1824 rows → 70 windows\n",
      "lab_person_2_cleaned.csv: 1955 rows → 76 windows\n",
      "\n",
      "TEST SUMMARY:\n",
      "Total windows: 577\n",
      "Feature vector size: 310 (57 sensors × 5 stats)\n",
      "\n",
      "================================================================================\n",
      "CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Training set:\n",
      "blinds     310\n",
      "hallway    523\n",
      "kitchen    531\n",
      "lab        388\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing set:\n",
      "blinds      32\n",
      "hallway    157\n",
      "kitchen    222\n",
      "lab        166\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "FEATURE SCALING\n",
      "================================================================================\n",
      "✓ Features scaled using StandardScaler\n",
      "\n",
      "================================================================================\n",
      "TRAINING GRADIENT BOOSTING CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "Hyperparameters:\n",
      "  - n_estimators: 200\n",
      "  - learning_rate: 0.05\n",
      "  - max_depth: 5\n",
      "  - random_state: 42\n",
      "\n",
      "Training...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2207            3.91m\n",
      "         2           1.1011            4.15m\n",
      "         3           0.9980            4.35m\n",
      "         4           0.9082            4.23m\n",
      "         5           0.8290            4.25m\n",
      "         6           0.7586            4.25m\n",
      "         7           0.6958            4.30m\n",
      "         8           0.6393            4.30m\n",
      "         9           0.5882            4.32m\n",
      "        10           0.5420            4.30m\n",
      "        20           0.2463            3.90m\n",
      "        30           0.1136            3.76m\n",
      "        40           0.0532            3.51m\n",
      "        50           0.0251            3.27m\n",
      "        60           0.0118            3.03m\n",
      "        70           0.0056            2.78m\n",
      "        80           0.0026            2.55m\n",
      "        90           0.0012            2.34m\n",
      "       100           0.0006            2.12m\n",
      "       200           0.0000            0.00s\n",
      "✓ Training complete!\n",
      "\n",
      "================================================================================\n",
      "EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Training Accuracy: 1.0000 (100.00%)\n",
      "Testing Accuracy: 0.8267 (82.67%)\n",
      "\n",
      "================================================================================\n",
      "DETAILED CLASSIFICATION REPORT (TEST SET)\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      blinds     0.4384    1.0000    0.6095        32\n",
      "     hallway     0.7500    0.9363    0.8329       157\n",
      "     kitchen     0.9342    0.6396    0.7594       222\n",
      "         lab     1.0000    0.9398    0.9689       166\n",
      "\n",
      "    accuracy                         0.8267       577\n",
      "   macro avg     0.7806    0.8789    0.7927       577\n",
      "weighted avg     0.8755    0.8267    0.8313       577\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONFUSION MATRIX (TEST SET)\n",
      "================================================================================\n",
      "         blinds  hallway  kitchen  lab\n",
      "blinds       32        0        0    0\n",
      "hallway       0      147       10    0\n",
      "kitchen      41       39      142    0\n",
      "lab           0       10        0  156\n",
      "\n",
      "Rows = Actual, Columns = Predicted\n",
      "\n",
      "================================================================================\n",
      "PER-CLASS ACCURACY\n",
      "================================================================================\n",
      "blinds      : 1.0000 (100.00%)\n",
      "hallway     : 0.9363 (93.63%)\n",
      "kitchen     : 0.6396 (63.96%)\n",
      "lab         : 0.9398 (93.98%)\n",
      "\n",
      "================================================================================\n",
      "TOP 20 MOST IMPORTANT FEATURES\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 240\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stat \u001b[38;5;129;01min\u001b[39;00m stats:\n\u001b[0;32m    238\u001b[0m         feature_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 240\u001b[0m feature_importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m: clf\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m    243\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importance\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING CLASSIFIER WITH FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define common columns\n",
    "COMMON_COLUMNS = [\n",
    "    '% Prims Clipped', '% Prims Trivially Rejected', '% Stalled on System Memory', \n",
    "    '% Texture L2 Miss', '% Vertex Fetch Stall', 'ALU / Fragment', 'ALU / Vertex', \n",
    "    'Average Polygon Area', 'Average Vertices / Polygon', 'Avg Bytes / Fragment', \n",
    "    'Avg Bytes / Vertex', 'Avg Preemption Delay', 'Clocks / Second', 'EFU / Fragment', \n",
    "    'EFU / Vertex', 'Fragment ALU Instructions / Sec (Full)', \n",
    "    'Fragment ALU Instructions / Sec (Half)', 'Fragment EFU Instructions / Second', \n",
    "    'Fragment Instructions / Second', 'Fragments Shaded / Second', 'GPU % Bus Busy', \n",
    "    'GPU Frequency', 'L1 Texture Cache Miss Per Pixel', 'Pre-clipped Polygons/Second', \n",
    "    'Preemptions / second', 'Read Total (Bytes/sec)', 'Reused Vertices / Second', \n",
    "    'SP Memory Read (Bytes/Second)', 'Texture Memory Read BW (Bytes/Second)', \n",
    "    'Textures / Fragment', 'Textures / Vertex', 'Vertex Instructions / Second', \n",
    "    'Vertex Memory Read (Bytes/Second)', 'Vertices Shaded / Second', \n",
    "    'Write Total (Bytes/sec)', 'app_gpu_ms', 'app_rss_mb', 'app_uss_mb', 'app_vss_mb', \n",
    "    'application_layer_count', 'application_prediction_milliseconds', 'available_memory_mb', \n",
    "    'cpu_frequency_mhz', 'cpu_level', 'cpu_util_0', 'cpu_util_1', 'cpu_util_2', \n",
    "    'cpu_util_3', 'cpu_util_4', 'cpu_util_5', 'display_refresh_rate', \n",
    "    'gpu_frequency_mhz', 'gpu_level', 'gpu_util', 'mem_frequency_mhz', \n",
    "    'stale_frames_per_second', 'timewarp_gpu_ms'\n",
    "]\n",
    "\n",
    "def create_overlapping_windows(df, window_size=75, stride=25):\n",
    "    \"\"\"\n",
    "    Create overlapping windows with stride.\n",
    "    stride=25 with window_size=75 gives 66% overlap and ~3x more windows\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for i in range(0, len(df) - window_size + 1, stride):\n",
    "        window = df.iloc[i:i+window_size].copy()\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def extract_statistical_features(window_df):\n",
    "    \"\"\"\n",
    "    Extract statistical features with emphasis on features that separate kitchen from blinds.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Top 5 most discriminative features - add MORE statistics for these\n",
    "    top_features = [\n",
    "        'ALU / Fragment',\n",
    "        'EFU / Fragment', \n",
    "        'Average Vertices / Polygon',\n",
    "        '% Stalled on System Memory',\n",
    "        'Fragment ALU Instructions / Sec (Half)'\n",
    "    ]\n",
    "    \n",
    "    # For top features, extract MANY statistics\n",
    "    for col in window_df.columns:\n",
    "        if col in top_features:\n",
    "            # Add 10 statistics for highly discriminative features\n",
    "            features.extend([\n",
    "                window_df[col].mean(),\n",
    "                window_df[col].std(),\n",
    "                window_df[col].max(),\n",
    "                window_df[col].min(),\n",
    "                window_df[col].median(),\n",
    "                window_df[col].quantile(0.25),\n",
    "                window_df[col].quantile(0.75),\n",
    "                (window_df[col].max() - window_df[col].min()),  # range\n",
    "                window_df[col].skew(),\n",
    "                window_df[col].kurtosis()\n",
    "            ])\n",
    "        else:\n",
    "            # For other features, keep original 5 statistics\n",
    "            features.extend([\n",
    "                window_df[col].mean(),\n",
    "                window_df[col].std(),\n",
    "                window_df[col].max(),\n",
    "                window_df[col].min(),\n",
    "                window_df[col].median()\n",
    "            ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def process_dataset(metadata_df, dataset_name, window_size=75, stride=25):\n",
    "    \"\"\"\n",
    "    Process dataset: load CSVs, create overlapping windows, extract features\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PROCESSING {dataset_name.upper()} SET\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        csv_path = Path(row['filepath'])\n",
    "        room_type = row['room_type']\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Select only common columns\n",
    "        available_common = [col for col in COMMON_COLUMNS if col in df.columns]\n",
    "        df = df[available_common]\n",
    "        \n",
    "        # Create overlapping windows\n",
    "        windows = create_overlapping_windows(df, window_size, stride)\n",
    "        \n",
    "        print(f\"{row['filename']}: {len(df)} rows → {len(windows)} windows\")\n",
    "        \n",
    "        # Extract features from each window\n",
    "        for window_df in windows:\n",
    "            features = extract_statistical_features(window_df)\n",
    "            all_features.append(features)\n",
    "            all_labels.append(room_type)\n",
    "    \n",
    "    X = np.array(all_features)\n",
    "    y = np.array(all_labels)\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()} SUMMARY:\")\n",
    "    print(f\"Total windows: {len(X)}\")\n",
    "    print(f\"Feature vector size: {X.shape[1]} (57 sensors × 5 stats)\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Load train and test metadata\n",
    "train_metadata = pd.read_csv(\"meta_scan_csvs/train_metadata.csv\")\n",
    "test_metadata = pd.read_csv(\"meta_scan_csvs/test_metadata.csv\")\n",
    "\n",
    "# Process training set\n",
    "X_train, y_train = process_dataset(\n",
    "    train_metadata, \n",
    "    \"train\",\n",
    "    window_size=75,\n",
    "    stride=25  # 66% overlap = ~3x more windows\n",
    ")\n",
    "\n",
    "# Process testing set\n",
    "X_test, y_test = process_dataset(\n",
    "    test_metadata, \n",
    "    \"test\",\n",
    "    window_size=75,\n",
    "    stride=25\n",
    ")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTraining set:\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "print(train_dist)\n",
    "print(f\"\\nTesting set:\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "print(test_dist)\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\" * 80)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✓ Features scaled using StandardScaler\")\n",
    "\n",
    "# Train Gradient Boosting Classifier\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING GRADIENT BOOSTING CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nHyperparameters:\")\n",
    "print(\"  - n_estimators: 200\")\n",
    "print(\"  - learning_rate: 0.05\")\n",
    "print(\"  - max_depth: 5\")\n",
    "print(\"  - random_state: 42\")\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print(\"✓ Training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "y_train_pred = clf.predict(X_train_scaled)\n",
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (TEST SET)\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRIX (TEST SET)\")\n",
    "print(\"=\" * 80)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "room_types = sorted(np.unique(y_test))\n",
    "cm_df = pd.DataFrame(cm, index=room_types, columns=room_types)\n",
    "print(cm_df)\n",
    "print(\"\\nRows = Actual, Columns = Predicted\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "for i, room_type in enumerate(room_types):\n",
    "    class_accuracy = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"{room_type:12s}: {class_accuracy:.4f} ({class_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Feature importance (top 20)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build feature names matching the new extraction function\n",
    "feature_names = []\n",
    "\n",
    "# Top 5 discriminative features get 10 stats each\n",
    "top_features = [\n",
    "    'ALU / Fragment',\n",
    "    'EFU / Fragment', \n",
    "    'Average Vertices / Polygon',\n",
    "    '% Stalled on System Memory',\n",
    "    'Fragment ALU Instructions / Sec (Half)'\n",
    "]\n",
    "\n",
    "top_stats = ['mean', 'std', 'max', 'min', 'median', 'q25', 'q75', 'range', 'skew', 'kurtosis']\n",
    "regular_stats = ['mean', 'std', 'max', 'min', 'median']\n",
    "\n",
    "for col in COMMON_COLUMNS:\n",
    "    if col in top_features:\n",
    "        for stat in top_stats:\n",
    "            feature_names.append(f\"{col}_{stat}\")\n",
    "    else:\n",
    "        for stat in regular_stats:\n",
    "            feature_names.append(f\"{col}_{stat}\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b9a539-8dc4-4f0e-8e69-6339ec1a6cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "TOP 20 FEATURES THAT SEPARATE KITCHEN FROM BLINDS\n",
      "====================================================================================================\n",
      "\n",
      "Rank  Feature                                            Sep Score    Cohen's d    Rel Diff    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "6     ALU / Fragment                                     0.7107       0.7107       1.0000      \n",
      "14    EFU / Fragment                                     0.6515       0.7818       0.8333      \n",
      "9     Average Vertices / Polygon                         0.3276       0.4488       0.7298      \n",
      "3     % Stalled on System Memory                         0.2666       0.6198       0.4301      \n",
      "17    Fragment ALU Instructions / Sec (Half)             0.1705       0.5324       0.3202      \n",
      "11    Avg Bytes / Vertex                                 0.1677       0.7118       0.2356      \n",
      "16    Fragment ALU Instructions / Sec (Full)             0.1670       0.5272       0.3167      \n",
      "28    SP Memory Read (Bytes/Second)                      0.1454       0.4653       0.3126      \n",
      "35    Write Total (Bytes/sec)                            0.0958       0.4627       0.2070      \n",
      "31    Textures / Vertex                                  0.0824       0.0905       0.9103      \n",
      "19    Fragment Instructions / Second                     0.0648       0.2424       0.2673      \n",
      "27    Reused Vertices / Second                           0.0638       0.5078       0.1256      \n",
      "30    Textures / Fragment                                0.0615       0.3893       0.1579      \n",
      "21    GPU % Bus Busy                                     0.0601       0.4792       0.1254      \n",
      "15    EFU / Vertex                                       0.0588       0.2682       0.2194      \n",
      "24    Pre-clipped Polygons/Second                        0.0528       0.4789       0.1103      \n",
      "7     ALU / Vertex                                       0.0494       0.3014       0.1639      \n",
      "18    Fragment EFU Instructions / Second                 0.0483       0.2695       0.1791      \n",
      "32    Vertex Instructions / Second                       0.0409       0.2509       0.1630      \n",
      "37    app_rss_mb                                         0.0329       0.2583       0.1275      \n",
      "\n",
      "====================================================================================================\n",
      "DETAILED VIEW: TOP 10 FEATURES\n",
      "====================================================================================================\n",
      "\n",
      "6. ALU / Fragment\n",
      "   Kitchen:       6.2953 ±    14.5168\n",
      "   Blinds:       -1.0000 ±     0.0000\n",
      "   Separation: 0.7107 (Cohen's d: 0.7107)\n",
      "\n",
      "14. EFU / Fragment\n",
      "   Kitchen:      -0.0909 ±     1.6445\n",
      "   Blinds:       -1.0000 ±     0.0000\n",
      "   Separation: 0.6515 (Cohen's d: 0.7818)\n",
      "\n",
      "9. Average Vertices / Polygon\n",
      "   Kitchen:       4.3429 ±    11.5429\n",
      "   Blinds:        0.6783 ±     0.2918\n",
      "   Separation: 0.3276 (Cohen's d: 0.4488)\n",
      "\n",
      "3. % Stalled on System Memory\n",
      "   Kitchen:       0.4334 ±     0.5744\n",
      "   Blinds:        0.1727 ±     0.1542\n",
      "   Separation: 0.2666 (Cohen's d: 0.6198)\n",
      "\n",
      "17. Fragment ALU Instructions / Sec (Half)\n",
      "   Kitchen: 37049387641.0628 ± 57886430659.8051\n",
      "   Blinds:  71951847532.0706 ± 72417088531.5966\n",
      "   Separation: 0.1705 (Cohen's d: 0.5324)\n",
      "\n",
      "11. Avg Bytes / Vertex\n",
      "   Kitchen:       7.0540 ±     5.0012\n",
      "   Blinds:        4.3635 ±     1.8871\n",
      "   Separation: 0.1677 (Cohen's d: 0.7118)\n",
      "\n",
      "16. Fragment ALU Instructions / Sec (Full)\n",
      "   Kitchen: 61930956556.8391 ± 66284647008.3969\n",
      "   Blinds:  32135456332.9889 ± 44659504129.9023\n",
      "   Separation: 0.1670 (Cohen's d: 0.5272)\n",
      "\n",
      "28. SP Memory Read (Bytes/Second)\n",
      "   Kitchen: 5113109.7314 ± 7254373.9880\n",
      "   Blinds:  2677881.0723 ± 1466214.0835\n",
      "   Separation: 0.1454 (Cohen's d: 0.4653)\n",
      "\n",
      "35. Write Total (Bytes/sec)\n",
      "   Kitchen: 1324949344.1578 ± 999848232.3820\n",
      "   Blinds:  870404711.0604 ± 964528405.4780\n",
      "   Separation: 0.0958 (Cohen's d: 0.4627)\n",
      "\n",
      "31. Textures / Vertex\n",
      "   Kitchen:      -0.0877 ±     0.9058\n",
      "   Blinds:       -0.0041 ±     0.9425\n",
      "   Separation: 0.0824 (Cohen's d: 0.0905)\n",
      "\n",
      "====================================================================================================\n",
      "RECOMMENDED ADDITIONAL STATISTICS FOR TOP 5 FEATURES\n",
      "====================================================================================================\n",
      "\n",
      "ALU / Fragment:\n",
      "   Best statistics: max, range, std, mean, skew\n",
      "\n",
      "EFU / Fragment:\n",
      "   Best statistics: max, range, std, skew, kurtosis\n",
      "\n",
      "Average Vertices / Polygon:\n",
      "   Best statistics: kurtosis, std, range, max, skew\n",
      "\n",
      "% Stalled on System Memory:\n",
      "   Best statistics: kurtosis, range, max, std, min\n",
      "\n",
      "Fragment ALU Instructions / Sec (Half):\n",
      "   Best statistics: median, skew, kurtosis, mean, std\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define common columns\n",
    "COMMON_COLUMNS = [\n",
    "    '% Prims Clipped', '% Prims Trivially Rejected', '% Stalled on System Memory', \n",
    "    '% Texture L2 Miss', '% Vertex Fetch Stall', 'ALU / Fragment', 'ALU / Vertex', \n",
    "    'Average Polygon Area', 'Average Vertices / Polygon', 'Avg Bytes / Fragment', \n",
    "    'Avg Bytes / Vertex', 'Avg Preemption Delay', 'Clocks / Second', 'EFU / Fragment', \n",
    "    'EFU / Vertex', 'Fragment ALU Instructions / Sec (Full)', \n",
    "    'Fragment ALU Instructions / Sec (Half)', 'Fragment EFU Instructions / Second', \n",
    "    'Fragment Instructions / Second', 'Fragments Shaded / Second', 'GPU % Bus Busy', \n",
    "    'GPU Frequency', 'L1 Texture Cache Miss Per Pixel', 'Pre-clipped Polygons/Second', \n",
    "    'Preemptions / second', 'Read Total (Bytes/sec)', 'Reused Vertices / Second', \n",
    "    'SP Memory Read (Bytes/Second)', 'Texture Memory Read BW (Bytes/Second)', \n",
    "    'Textures / Fragment', 'Textures / Vertex', 'Vertex Instructions / Second', \n",
    "    'Vertex Memory Read (Bytes/Second)', 'Vertices Shaded / Second', \n",
    "    'Write Total (Bytes/sec)', 'app_gpu_ms', 'app_rss_mb', 'app_uss_mb', 'app_vss_mb', \n",
    "    'application_layer_count', 'application_prediction_milliseconds', 'available_memory_mb', \n",
    "    'cpu_frequency_mhz', 'cpu_level', 'cpu_util_0', 'cpu_util_1', 'cpu_util_2', \n",
    "    'cpu_util_3', 'cpu_util_4', 'cpu_util_5', 'display_refresh_rate', \n",
    "    'gpu_frequency_mhz', 'gpu_level', 'gpu_util', 'mem_frequency_mhz', \n",
    "    'stale_frames_per_second', 'timewarp_gpu_ms'\n",
    "]\n",
    "\n",
    "# Load all kitchen files\n",
    "kitchen_path = Path(\"meta_scan_csvs/cleaned/kitchen\")\n",
    "kitchen_dfs = []\n",
    "for csv_file in kitchen_path.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    available = [col for col in COMMON_COLUMNS if col in df.columns]\n",
    "    kitchen_dfs.append(df[available])\n",
    "\n",
    "kitchen_all = pd.concat(kitchen_dfs, ignore_index=True)\n",
    "\n",
    "# Load all blinds files\n",
    "blinds_path = Path(\"meta_scan_csvs/cleaned/blinds\")\n",
    "blinds_dfs = []\n",
    "for csv_file in blinds_path.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    available = [col for col in COMMON_COLUMNS if col in df.columns]\n",
    "    blinds_dfs.append(df[available])\n",
    "\n",
    "blinds_all = pd.concat(blinds_dfs, ignore_index=True)\n",
    "\n",
    "# Compute statistics for each feature\n",
    "feature_separation = []\n",
    "\n",
    "for feature in COMMON_COLUMNS:\n",
    "    if feature in kitchen_all.columns and feature in blinds_all.columns:\n",
    "        # Get statistics\n",
    "        k_mean = kitchen_all[feature].mean()\n",
    "        k_std = kitchen_all[feature].std()\n",
    "        b_mean = blinds_all[feature].mean()\n",
    "        b_std = blinds_all[feature].std()\n",
    "        \n",
    "        # Calculate separation metrics\n",
    "        # 1. Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt((k_std**2 + b_std**2) / 2)\n",
    "        cohens_d = abs(k_mean - b_mean) / (pooled_std + 1e-10)\n",
    "        \n",
    "        # 2. Relative difference\n",
    "        rel_diff = abs(k_mean - b_mean) / (abs(k_mean) + abs(b_mean) + 1e-10)\n",
    "        \n",
    "        # 3. Combined score (higher = better separation)\n",
    "        separation_score = cohens_d * rel_diff\n",
    "        \n",
    "        feature_separation.append({\n",
    "            'feature': feature,\n",
    "            'kitchen_mean': k_mean,\n",
    "            'kitchen_std': k_std,\n",
    "            'blinds_mean': b_mean,\n",
    "            'blinds_std': b_std,\n",
    "            'cohens_d': cohens_d,\n",
    "            'relative_diff': rel_diff,\n",
    "            'separation_score': separation_score\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "sep_df = pd.DataFrame(feature_separation).sort_values('separation_score', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"TOP 20 FEATURES THAT SEPARATE KITCHEN FROM BLINDS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n{'Rank':<5} {'Feature':<50} {'Sep Score':<12} {'Cohen\\'s d':<12} {'Rel Diff':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for i, row in sep_df.head(20).iterrows():\n",
    "    print(f\"{i+1:<5} {row['feature']:<50} {row['separation_score']:<12.4f} {row['cohens_d']:<12.4f} {row['relative_diff']:<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"DETAILED VIEW: TOP 10 FEATURES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, row in sep_df.head(10).iterrows():\n",
    "    print(f\"\\n{i+1}. {row['feature']}\")\n",
    "    print(f\"   Kitchen: {row['kitchen_mean']:12.4f} ± {row['kitchen_std']:10.4f}\")\n",
    "    print(f\"   Blinds:  {row['blinds_mean']:12.4f} ± {row['blinds_std']:10.4f}\")\n",
    "    print(f\"   Separation: {row['separation_score']:.4f} (Cohen's d: {row['cohens_d']:.4f})\")\n",
    "\n",
    "# Also check which stats (mean, std, max, etc.) work best for these features\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"RECOMMENDED ADDITIONAL STATISTICS FOR TOP 5 FEATURES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, row in sep_df.head(5).iterrows():\n",
    "    feature = row['feature']\n",
    "    print(f\"\\n{feature}:\")\n",
    "    \n",
    "    # Check different statistics\n",
    "    k_vals = kitchen_all[feature].dropna()\n",
    "    b_vals = blinds_all[feature].dropna()\n",
    "    \n",
    "    stats = {\n",
    "        'mean': (k_vals.mean(), b_vals.mean()),\n",
    "        'std': (k_vals.std(), b_vals.std()),\n",
    "        'median': (k_vals.median(), b_vals.median()),\n",
    "        'q25': (k_vals.quantile(0.25), b_vals.quantile(0.25)),\n",
    "        'q75': (k_vals.quantile(0.75), b_vals.quantile(0.75)),\n",
    "        'max': (k_vals.max(), b_vals.max()),\n",
    "        'min': (k_vals.min(), b_vals.min()),\n",
    "        'range': (k_vals.max() - k_vals.min(), b_vals.max() - b_vals.min()),\n",
    "        'skew': (k_vals.skew(), b_vals.skew()),\n",
    "        'kurtosis': (k_vals.kurtosis(), b_vals.kurtosis())\n",
    "    }\n",
    "    \n",
    "    # Calculate separation for each stat\n",
    "    stat_sep = {}\n",
    "    for stat_name, (k_val, b_val) in stats.items():\n",
    "        diff = abs(k_val - b_val) / (abs(k_val) + abs(b_val) + 1e-10)\n",
    "        stat_sep[stat_name] = diff\n",
    "    \n",
    "    # Sort and show top 5\n",
    "    top_stats = sorted(stat_sep.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"   Best statistics: {', '.join([s[0] for s in top_stats])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f517d79f-3132-4e43-b419-492a5a888d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded blinds_10_cleaned.csv: (1719, 70)\n",
      "  Loaded blinds_6_cleaned.csv: (1365, 70)\n",
      "  Loaded blinds_7_cleaned.csv: (301, 58)\n",
      "  Loaded blinds_8_cleaned.csv: (312, 59)\n",
      "  Loaded blinds_9_cleaned.csv: (290, 70)\n",
      "  Loaded blinds_motion_1_cleaned.csv: (1838, 66)\n",
      "  Loaded blinds_motion_2_cleaned.csv: (329, 65)\n",
      "  Loaded blinds_motion_3_cleaned.csv: (335, 59)\n",
      "  Loaded blinds_motion_4_cleaned.csv: (312, 58)\n",
      "  Loaded blinds_motion_5_cleaned.csv: (327, 60)\n",
      "  Loaded blinds_object_1_cleaned.csv: (1821, 62)\n",
      "  Loaded blinds_object_2_cleaned.csv: (327, 59)\n",
      "  Loaded blinds_object_3_cleaned.csv: (327, 59)\n",
      "  Loaded blinds_object_4_cleaned.csv: (325, 59)\n",
      "  Loaded blinds_object_5_cleaned.csv: (1827, 59)\n",
      "  Loaded blinds_person_1_cleaned.csv: (350, 68)\n",
      "  Loaded blinds_person_2_cleaned.csv: (313, 64)\n",
      "  Loaded blinds_person_3_cleaned.csv: (326, 58)\n",
      "  Loaded blinds_person_4_cleaned.csv: (312, 59)\n",
      "  Loaded blinds_person_5_cleaned.csv: (327, 59)\n",
      "  Loaded blinds_up_10_cleaned.csv: (319, 58)\n",
      "  Loaded blinds_up_6_cleaned.csv: (295, 70)\n",
      "  Loaded blinds_up_7_cleaned.csv: (1711, 69)\n",
      "  Loaded blinds_up_8_cleaned.csv: (305, 59)\n",
      "  Loaded blinds_up_9_cleaned.csv: (310, 59)\n",
      "  Loaded blinds_up_motion_10_cleaned.csv: (311, 58)\n",
      "  Loaded blinds_up_motion_6_cleaned.csv: (298, 59)\n",
      "  Loaded blinds_up_motion_7_cleaned.csv: (1697, 60)\n",
      "  Loaded blinds_up_motion_8_cleaned.csv: (371, 59)\n",
      "  Loaded blinds_up_motion_9_cleaned.csv: (295, 59)\n",
      "  Loaded blinds_up_object_1_cleaned.csv: (1755, 66)\n",
      "  Loaded blinds_up_object_2_cleaned.csv: (1765, 59)\n",
      "  Loaded blinds_up_object_3_cleaned.csv: (1891, 60)\n",
      "  Loaded blinds_up_object_4_cleaned.csv: (313, 59)\n",
      "  Loaded blinds_up_object_5_cleaned.csv: (1836, 60)\n",
      "  Loaded blinds_up_person_10_cleaned.csv: (318, 59)\n",
      "  Loaded blinds_up_person_6_cleaned.csv: (309, 70)\n",
      "  Loaded blinds_up_person_7_cleaned.csv: (319, 68)\n",
      "  Loaded blinds_up_person_8_cleaned.csv: (1851, 60)\n",
      "  Loaded blinds_up_person_9_cleaned.csv: (304, 59)\n",
      "  Loaded hallway_1_cleaned.csv: (300, 59)\n",
      "  Loaded hallway_2_cleaned.csv: (305, 59)\n",
      "  Loaded hallway_3_cleaned.csv: (306, 59)\n",
      "  Loaded hallway_4_cleaned.csv: (320, 58)\n",
      "  Loaded hallway_5_cleaned.csv: (1493, 70)\n",
      "  Loaded hallway_motion_1_cleaned.csv: (329, 59)\n",
      "  Loaded hallway_motion_2_cleaned.csv: (327, 60)\n",
      "  Loaded hallway_motion_3_cleaned.csv: (313, 58)\n",
      "  Loaded hallway_motion_4_cleaned.csv: (1869, 60)\n",
      "  Loaded hallway_motion_5_cleaned.csv: (314, 59)\n",
      "  Loaded hallway_object_1_cleaned.csv: (2024, 69)\n",
      "  Loaded hallway_object_2_cleaned.csv: (1712, 70)\n",
      "  Loaded hallway_object_3_cleaned.csv: (1739, 60)\n",
      "  Loaded hallway_object_4_cleaned.csv: (304, 59)\n",
      "  Loaded hallway_object_5_cleaned.csv: (306, 59)\n",
      "  Loaded hallway_person_1_cleaned.csv: (340, 69)\n",
      "  Loaded hallway_person_2_cleaned.csv: (1820, 69)\n",
      "  Loaded hallway_person_3_cleaned.csv: (345, 58)\n",
      "  Loaded hallway_person_4_cleaned.csv: (1977, 59)\n",
      "  Loaded hallway_person_5_cleaned.csv: (1799, 59)\n",
      "  Loaded kitchen_10_cleaned.csv: (2064, 70)\n",
      "  Loaded kitchen_1_cleaned.csv: (320, 58)\n",
      "  Loaded kitchen_2_cleaned.csv: (296, 59)\n",
      "  Loaded kitchen_3_cleaned.csv: (312, 69)\n",
      "  Loaded kitchen_4_cleaned.csv: (1668, 68)\n",
      "  Loaded kitchen_6_cleaned.csv: (250, 63)\n",
      "  Loaded kitchen_9_cleaned.csv: (1971, 69)\n",
      "  Loaded kitchen_motion_1_cleaned.csv: (313, 65)\n",
      "  Loaded kitchen_motion_2_cleaned.csv: (1647, 67)\n",
      "  Loaded kitchen_motion_3_cleaned.csv: (1837, 60)\n",
      "  Loaded kitchen_motion_4_cleaned.csv: (338, 58)\n",
      "  Loaded kitchen_motion_5_cleaned.csv: (342, 59)\n",
      "  Loaded kitchen_object_1_cleaned.csv: (1748, 65)\n",
      "  Loaded kitchen_object_2_cleaned.csv: (326, 66)\n",
      "  Loaded kitchen_object_3_cleaned.csv: (1839, 60)\n",
      "  Loaded kitchen_object_4_cleaned.csv: (1839, 59)\n",
      "  Loaded kitchen_object_5_cleaned.csv: (340, 60)\n",
      "  Loaded kitchen_person_2_cleaned.csv: (335, 67)\n",
      "  Loaded kitchen_person_3_cleaned.csv: (1754, 59)\n",
      "  Loaded kitchen_person_4_cleaned.csv: (308, 59)\n",
      "  Loaded kitchen_person_5_cleaned.csv: (322, 59)\n",
      "  Loaded lab_1_cleaned.csv: (1749, 69)\n",
      "  Loaded lab_2_cleaned.csv: (319, 68)\n",
      "  Loaded lab_3_cleaned.csv: (312, 58)\n",
      "  Loaded lab_4_cleaned.csv: (312, 58)\n",
      "  Loaded lab_5_cleaned.csv: (1717, 60)\n",
      "  Loaded lab_motion_1_cleaned.csv: (300, 58)\n",
      "  Loaded lab_motion_2_cleaned.csv: (317, 69)\n",
      "  Loaded lab_motion_3_cleaned.csv: (330, 70)\n",
      "  Loaded lab_motion_4_cleaned.csv: (322, 69)\n",
      "  Loaded lab_motion_5_cleaned.csv: (1859, 69)\n",
      "  Loaded lab_object_1_cleaned.csv: (1850, 69)\n",
      "  Loaded lab_object_2_cleaned.csv: (1824, 59)\n",
      "  Loaded lab_object_3_cleaned.csv: (313, 58)\n",
      "  Loaded lab_object_4_cleaned.csv: (297, 59)\n",
      "  Loaded lab_object_5_cleaned.csv: (296, 58)\n",
      "  Loaded lab_person_1_cleaned.csv: (341, 70)\n",
      "  Loaded lab_person_2_cleaned.csv: (1955, 70)\n",
      "  Loaded lab_person_3_cleaned.csv: (328, 67)\n",
      "  Loaded lab_person_5_cleaned.csv: (309, 58)\n",
      "================================================================================\n",
      "DATA INVENTORY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total CSV files: 100\n",
      "\n",
      "Room types (5): ['blinds', 'blinds_up', 'hallway', 'kitchen', 'lab']\n",
      "Scan types (4): ['base', 'motion', 'object', 'person']\n",
      "\n",
      "================================================================================\n",
      "COUNTS BY ROOM TYPE AND SCAN TYPE\n",
      "================================================================================\n",
      "scan_type  base  motion  object  person\n",
      "room_type                              \n",
      "blinds        5       5       5       5\n",
      "blinds_up     5       5       5       5\n",
      "hallway       5       5       5       5\n",
      "kitchen       7       5       5       4\n",
      "lab           5       5       5       4\n",
      "\n",
      "Total scans per room type:\n",
      "room_type\n",
      "blinds       20\n",
      "blinds_up    20\n",
      "hallway      20\n",
      "kitchen      21\n",
      "lab          19\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "ROW COUNTS STATISTICS BY ROOM TYPE\n",
      "================================================================================\n",
      "           count    mean     std    min     25%    50%      75%     max\n",
      "room_type                                                              \n",
      "blinds      20.0  669.15  625.87  290.0  312.75  327.0   603.75  1838.0\n",
      "blinds_up   20.0  828.65  722.54  295.0  308.00  318.5  1722.00  1891.0\n",
      "hallway     20.0  912.10  754.08  300.0  311.25  334.5  1754.00  2024.0\n",
      "kitchen     21.0  960.43  766.52  250.0  320.00  340.0  1754.00  2064.0\n",
      "lab         19.0  792.11  722.88  296.0  312.00  322.0  1733.00  1955.0\n",
      "\n",
      "================================================================================\n",
      "SAMPLE FILES (sorted by room_type, scan_type, trial)\n",
      "================================================================================\n",
      "                       filename room_type scan_type  trial_number  num_rows\n",
      "1          blinds_6_cleaned.csv    blinds      base             6      1365\n",
      "2          blinds_7_cleaned.csv    blinds      base             7       301\n",
      "3          blinds_8_cleaned.csv    blinds      base             8       312\n",
      "4          blinds_9_cleaned.csv    blinds      base             9       290\n",
      "0         blinds_10_cleaned.csv    blinds      base            10      1719\n",
      "5   blinds_motion_1_cleaned.csv    blinds    motion             1      1838\n",
      "6   blinds_motion_2_cleaned.csv    blinds    motion             2       329\n",
      "7   blinds_motion_3_cleaned.csv    blinds    motion             3       335\n",
      "8   blinds_motion_4_cleaned.csv    blinds    motion             4       312\n",
      "9   blinds_motion_5_cleaned.csv    blinds    motion             5       327\n",
      "10  blinds_object_1_cleaned.csv    blinds    object             1      1821\n",
      "11  blinds_object_2_cleaned.csv    blinds    object             2       327\n",
      "12  blinds_object_3_cleaned.csv    blinds    object             3       327\n",
      "13  blinds_object_4_cleaned.csv    blinds    object             4       325\n",
      "14  blinds_object_5_cleaned.csv    blinds    object             5      1827\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "Expected: 100 scans (was 105, removed 5 with insufficient features)\n",
      "Actual: 100 scans\n",
      "✓ Count matches!\n",
      "\n",
      "✓ Saved complete inventory to: meta_scan_csvs/data_inventory.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def parse_filename(filepath):\n",
    "    \"\"\"\n",
    "    Parse filename to extract room_type, scan_type, and trial_number\n",
    "    Examples:\n",
    "    - blinds_1_cleaned.csv -> room: blinds, scan: base, trial: 1\n",
    "    - blinds_up_1_cleaned.csv -> room: blinds_up, scan: base, trial: 1\n",
    "    - kitchen_motion_3_cleaned.csv -> room: kitchen, scan: motion, trial: 3\n",
    "    \"\"\"\n",
    "    filename = filepath.stem.replace('_cleaned', '')\n",
    "    \n",
    "    # Handle blinds_up as a special case (two-word room type)\n",
    "    if filename.startswith('blinds_up'):\n",
    "        room_type = 'blinds_up'\n",
    "        remainder = filename.replace('blinds_up_', '', 1)\n",
    "    else:\n",
    "        # Split and take first part as room type\n",
    "        parts = filename.split('_')\n",
    "        room_type = parts[0]\n",
    "        remainder = '_'.join(parts[1:])\n",
    "    \n",
    "    # Now parse the remainder for scan type and trial\n",
    "    remainder_parts = remainder.split('_')\n",
    "    \n",
    "    if len(remainder_parts) == 1:\n",
    "        # Base scan (just a number)\n",
    "        scan_type = 'base'\n",
    "        trial_number = int(remainder_parts[0])\n",
    "    elif len(remainder_parts) == 2:\n",
    "        # Scan type + number (e.g., motion_3)\n",
    "        scan_type = remainder_parts[0]\n",
    "        trial_number = int(remainder_parts[1])\n",
    "    else:\n",
    "        scan_type = 'unknown'\n",
    "        trial_number = -1\n",
    "    \n",
    "    return room_type, scan_type, trial_number\n",
    "\n",
    "# Scan all CSV files\n",
    "cleaned_path = Path(\"meta_scan_csvs/cleaned\")\n",
    "csv_files = list(cleaned_path.rglob(\"*.csv\"))\n",
    "\n",
    "# Create metadata inventory\n",
    "metadata = []\n",
    "for csv_file in csv_files:\n",
    "    room_type, scan_type, trial_number = parse_filename(csv_file)\n",
    "    \n",
    "    # Get number of rows\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"  Loaded {csv_file.name}: {df.shape}\")\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    metadata.append({\n",
    "        'filepath': str(csv_file),\n",
    "        'filename': csv_file.name,\n",
    "        'room_type': room_type,\n",
    "        'scan_type': scan_type,\n",
    "        'trial_number': trial_number,\n",
    "        'num_rows': num_rows,\n",
    "        'folder': csv_file.parent.name\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "# Display summary\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA INVENTORY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal CSV files: {len(metadata_df)}\")\n",
    "print(f\"\\nRoom types ({len(metadata_df['room_type'].unique())}): {sorted(metadata_df['room_type'].unique())}\")\n",
    "print(f\"Scan types ({len(metadata_df['scan_type'].unique())}): {sorted(metadata_df['scan_type'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COUNTS BY ROOM TYPE AND SCAN TYPE\")\n",
    "print(\"=\" * 80)\n",
    "pivot = metadata_df.groupby(['room_type', 'scan_type']).size().unstack(fill_value=0)\n",
    "print(pivot)\n",
    "print(f\"\\nTotal scans per room type:\")\n",
    "print(pivot.sum(axis=1))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROW COUNTS STATISTICS BY ROOM TYPE\")\n",
    "print(\"=\" * 80)\n",
    "print(metadata_df.groupby('room_type')['num_rows'].describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE FILES (sorted by room_type, scan_type, trial)\")\n",
    "print(\"=\" * 80)\n",
    "sample = metadata_df.sort_values(['room_type', 'scan_type', 'trial_number'])\n",
    "print(sample[['filename', 'room_type', 'scan_type', 'trial_number', 'num_rows']].head(15))\n",
    "\n",
    "# Verify we have exactly 105 scans\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Expected: 100 scans\")\n",
    "print(f\"Actual: {len(metadata_df)} scans\")\n",
    "if len(metadata_df) == 100:\n",
    "    print(\"✓ Count matches!\")\n",
    "else:\n",
    "    print(\"⚠ Count mismatch - please review\")\n",
    "\n",
    "# Save metadata to CSV for reference\n",
    "metadata_df.to_csv(\"meta_scan_csvs/data_inventory.csv\", index=False)\n",
    "print(f\"\\n✓ Saved complete inventory to: meta_scan_csvs/data_inventory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74789404-1895-4811-b808-ad8f5cecc492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYZING SENSOR COLUMNS ACROSS ALL 100 CSVs\n",
      "================================================================================\n",
      "blinds_10_cleaned.csv: 70 columns\n",
      "blinds_6_cleaned.csv: 70 columns\n",
      "blinds_7_cleaned.csv: 58 columns\n",
      "blinds_8_cleaned.csv: 59 columns\n",
      "blinds_9_cleaned.csv: 70 columns\n",
      "blinds_motion_1_cleaned.csv: 66 columns\n",
      "blinds_motion_2_cleaned.csv: 65 columns\n",
      "blinds_motion_3_cleaned.csv: 59 columns\n",
      "blinds_motion_4_cleaned.csv: 58 columns\n",
      "blinds_motion_5_cleaned.csv: 60 columns\n",
      "blinds_object_1_cleaned.csv: 62 columns\n",
      "blinds_object_2_cleaned.csv: 59 columns\n",
      "blinds_object_3_cleaned.csv: 59 columns\n",
      "blinds_object_4_cleaned.csv: 59 columns\n",
      "blinds_object_5_cleaned.csv: 59 columns\n",
      "blinds_person_1_cleaned.csv: 68 columns\n",
      "blinds_person_2_cleaned.csv: 64 columns\n",
      "blinds_person_3_cleaned.csv: 58 columns\n",
      "blinds_person_4_cleaned.csv: 59 columns\n",
      "blinds_person_5_cleaned.csv: 59 columns\n",
      "blinds_up_10_cleaned.csv: 58 columns\n",
      "blinds_up_6_cleaned.csv: 70 columns\n",
      "blinds_up_7_cleaned.csv: 69 columns\n",
      "blinds_up_8_cleaned.csv: 59 columns\n",
      "blinds_up_9_cleaned.csv: 59 columns\n",
      "blinds_up_motion_10_cleaned.csv: 58 columns\n",
      "blinds_up_motion_6_cleaned.csv: 59 columns\n",
      "blinds_up_motion_7_cleaned.csv: 60 columns\n",
      "blinds_up_motion_8_cleaned.csv: 59 columns\n",
      "blinds_up_motion_9_cleaned.csv: 59 columns\n",
      "blinds_up_object_1_cleaned.csv: 66 columns\n",
      "blinds_up_object_2_cleaned.csv: 59 columns\n",
      "blinds_up_object_3_cleaned.csv: 60 columns\n",
      "blinds_up_object_4_cleaned.csv: 59 columns\n",
      "blinds_up_object_5_cleaned.csv: 60 columns\n",
      "blinds_up_person_10_cleaned.csv: 59 columns\n",
      "blinds_up_person_6_cleaned.csv: 70 columns\n",
      "blinds_up_person_7_cleaned.csv: 68 columns\n",
      "blinds_up_person_8_cleaned.csv: 60 columns\n",
      "blinds_up_person_9_cleaned.csv: 59 columns\n",
      "hallway_1_cleaned.csv: 59 columns\n",
      "hallway_2_cleaned.csv: 59 columns\n",
      "hallway_3_cleaned.csv: 59 columns\n",
      "hallway_4_cleaned.csv: 58 columns\n",
      "hallway_5_cleaned.csv: 70 columns\n",
      "hallway_motion_1_cleaned.csv: 59 columns\n",
      "hallway_motion_2_cleaned.csv: 60 columns\n",
      "hallway_motion_3_cleaned.csv: 58 columns\n",
      "hallway_motion_4_cleaned.csv: 60 columns\n",
      "hallway_motion_5_cleaned.csv: 59 columns\n",
      "hallway_object_1_cleaned.csv: 69 columns\n",
      "hallway_object_2_cleaned.csv: 70 columns\n",
      "hallway_object_3_cleaned.csv: 60 columns\n",
      "hallway_object_4_cleaned.csv: 59 columns\n",
      "hallway_object_5_cleaned.csv: 59 columns\n",
      "hallway_person_1_cleaned.csv: 69 columns\n",
      "hallway_person_2_cleaned.csv: 69 columns\n",
      "hallway_person_3_cleaned.csv: 58 columns\n",
      "hallway_person_4_cleaned.csv: 59 columns\n",
      "hallway_person_5_cleaned.csv: 59 columns\n",
      "kitchen_10_cleaned.csv: 70 columns\n",
      "kitchen_1_cleaned.csv: 58 columns\n",
      "kitchen_2_cleaned.csv: 59 columns\n",
      "kitchen_3_cleaned.csv: 69 columns\n",
      "kitchen_4_cleaned.csv: 68 columns\n",
      "kitchen_6_cleaned.csv: 63 columns\n",
      "kitchen_9_cleaned.csv: 69 columns\n",
      "kitchen_motion_1_cleaned.csv: 65 columns\n",
      "kitchen_motion_2_cleaned.csv: 67 columns\n",
      "kitchen_motion_3_cleaned.csv: 60 columns\n",
      "kitchen_motion_4_cleaned.csv: 58 columns\n",
      "kitchen_motion_5_cleaned.csv: 59 columns\n",
      "kitchen_object_1_cleaned.csv: 65 columns\n",
      "kitchen_object_2_cleaned.csv: 66 columns\n",
      "kitchen_object_3_cleaned.csv: 60 columns\n",
      "kitchen_object_4_cleaned.csv: 59 columns\n",
      "kitchen_object_5_cleaned.csv: 60 columns\n",
      "kitchen_person_2_cleaned.csv: 67 columns\n",
      "kitchen_person_3_cleaned.csv: 59 columns\n",
      "kitchen_person_4_cleaned.csv: 59 columns\n",
      "kitchen_person_5_cleaned.csv: 59 columns\n",
      "lab_1_cleaned.csv: 69 columns\n",
      "lab_2_cleaned.csv: 68 columns\n",
      "lab_3_cleaned.csv: 58 columns\n",
      "lab_4_cleaned.csv: 58 columns\n",
      "lab_5_cleaned.csv: 60 columns\n",
      "lab_motion_1_cleaned.csv: 58 columns\n",
      "lab_motion_2_cleaned.csv: 69 columns\n",
      "lab_motion_3_cleaned.csv: 70 columns\n",
      "lab_motion_4_cleaned.csv: 69 columns\n",
      "lab_motion_5_cleaned.csv: 69 columns\n",
      "lab_object_1_cleaned.csv: 69 columns\n",
      "lab_object_2_cleaned.csv: 59 columns\n",
      "lab_object_3_cleaned.csv: 58 columns\n",
      "lab_object_4_cleaned.csv: 59 columns\n",
      "lab_object_5_cleaned.csv: 58 columns\n",
      "lab_person_1_cleaned.csv: 70 columns\n",
      "lab_person_2_cleaned.csv: 70 columns\n",
      "lab_person_3_cleaned.csv: 67 columns\n",
      "lab_person_5_cleaned.csv: 58 columns\n",
      "\n",
      "================================================================================\n",
      "COMMON COLUMNS ACROSS ALL 100 CSVs: 54\n",
      "================================================================================\n",
      "\n",
      "COMMON_COLUMNS = [\n",
      "    '% Prims Clipped',\n",
      "    '% Prims Trivially Rejected',\n",
      "    '% Stalled on System Memory',\n",
      "    '% Texture L2 Miss',\n",
      "    '% Vertex Fetch Stall',\n",
      "    'ALU / Fragment',\n",
      "    'ALU / Vertex',\n",
      "    'Average Polygon Area',\n",
      "    'Average Vertices / Polygon',\n",
      "    'Avg Bytes / Fragment',\n",
      "    'Avg Bytes / Vertex',\n",
      "    'Avg Preemption Delay',\n",
      "    'Clocks / Second',\n",
      "    'EFU / Fragment',\n",
      "    'EFU / Vertex',\n",
      "    'Fragment ALU Instructions / Sec (Full)',\n",
      "    'Fragment ALU Instructions / Sec (Half)',\n",
      "    'Fragment EFU Instructions / Second',\n",
      "    'Fragment Instructions / Second',\n",
      "    'Fragments Shaded / Second',\n",
      "    'GPU % Bus Busy',\n",
      "    'GPU Frequency',\n",
      "    'L1 Texture Cache Miss Per Pixel',\n",
      "    'Pre-clipped Polygons/Second',\n",
      "    'Preemptions / second',\n",
      "    'Read Total (Bytes/sec)',\n",
      "    'Reused Vertices / Second',\n",
      "    'SP Memory Read (Bytes/Second)',\n",
      "    'Texture Memory Read BW (Bytes/Second)',\n",
      "    'Textures / Fragment',\n",
      "    'Textures / Vertex',\n",
      "    'Time (s)',\n",
      "    'Vertex Instructions / Second',\n",
      "    'Vertex Memory Read (Bytes/Second)',\n",
      "    'Vertices Shaded / Second',\n",
      "    'Write Total (Bytes/sec)',\n",
      "    'app_rss_mb',\n",
      "    'app_uss_mb',\n",
      "    'app_vss_mb',\n",
      "    'available_memory_mb',\n",
      "    'cpu_frequency_mhz',\n",
      "    'cpu_level',\n",
      "    'cpu_util_0',\n",
      "    'cpu_util_1',\n",
      "    'cpu_util_2',\n",
      "    'cpu_util_3',\n",
      "    'cpu_util_4',\n",
      "    'cpu_util_5',\n",
      "    'display_refresh_rate',\n",
      "    'gpu_frequency_mhz',\n",
      "    'gpu_level',\n",
      "    'gpu_util',\n",
      "    'mem_frequency_mhz',\n",
      "    'timewarp_gpu_ms',\n",
      "]\n",
      "\n",
      "================================================================================\n",
      "Column count range: 58 to 70\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cleaned_path = Path(\"meta_scan_csvs/cleaned\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING SENSOR COLUMNS ACROSS ALL 100 CSVs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_columns_sets = []\n",
    "\n",
    "for csv_file in cleaned_path.rglob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    cols = set(df.columns)\n",
    "    all_columns_sets.append(cols)\n",
    "    print(f\"{csv_file.name}: {len(cols)} columns\")\n",
    "\n",
    "# Find common columns across ALL CSVs\n",
    "common_columns = set.intersection(*all_columns_sets)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"COMMON COLUMNS ACROSS ALL 100 CSVs: {len(common_columns)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print in list format ready for copying into code\n",
    "print(\"\\nCOMMON_COLUMNS = [\")\n",
    "for col in sorted(common_columns):\n",
    "    print(f\"    '{col}',\")\n",
    "print(\"]\")\n",
    "\n",
    "# Find the range of column counts\n",
    "all_column_counts = [len(s) for s in all_columns_sets]\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"Column count range: {min(all_column_counts)} to {max(all_column_counts)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5d064d-fb88-4bf9-99e1-d2ad7a0758ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sktime in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (0.39.0)\n",
      "Requirement already satisfied: joblib<1.6,>=1.2.0 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (1.4.2)\n",
      "Requirement already satisfied: numpy<2.4,>=1.21 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (24.1)\n",
      "Requirement already satisfied: pandas<2.4.0,>=1.1 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (2.2.2)\n",
      "Requirement already satisfied: scikit-base<0.13.0,>=0.6.1 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (0.12.6)\n",
      "Requirement already satisfied: scikit-learn<1.8.0,>=0.24 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (1.5.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from sktime) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from pandas<2.4.0,>=1.1->sktime) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from pandas<2.4.0,>=1.1->sktime) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from pandas<2.4.0,>=1.1->sktime) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from scikit-learn<1.8.0,>=0.24->sktime) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gkamt\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.4.0,>=1.1->sktime) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sktime --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd53c76-2ab8-4754-8839-c60e690dcec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MANUAL STRATIFIED TRAIN/TEST SPLIT (~80/20)\n",
      "================================================================================\n",
      "blinds_base: 5 total → 4 train, 1 test\n",
      "blinds_motion: 5 total → 4 train, 1 test\n",
      "blinds_object: 5 total → 4 train, 1 test\n",
      "blinds_person: 5 total → 4 train, 1 test\n",
      "blinds_up_base: 5 total → 4 train, 1 test\n",
      "blinds_up_motion: 5 total → 4 train, 1 test\n",
      "blinds_up_object: 5 total → 4 train, 1 test\n",
      "blinds_up_person: 5 total → 4 train, 1 test\n",
      "hallway_base: 5 total → 4 train, 1 test\n",
      "hallway_motion: 5 total → 4 train, 1 test\n",
      "hallway_object: 5 total → 4 train, 1 test\n",
      "hallway_person: 5 total → 4 train, 1 test\n",
      "kitchen_base: 7 total → 6 train, 1 test\n",
      "kitchen_motion: 5 total → 4 train, 1 test\n",
      "kitchen_object: 5 total → 4 train, 1 test\n",
      "kitchen_person: 4 total → 3 train, 1 test\n",
      "lab_base: 5 total → 4 train, 1 test\n",
      "lab_motion: 5 total → 4 train, 1 test\n",
      "lab_object: 5 total → 4 train, 1 test\n",
      "lab_person: 4 total → 3 train, 1 test\n",
      "\n",
      "Total files: 100\n",
      "Training files: 80 (80.0%)\n",
      "Testing files: 20 (20.0%)\n",
      "\n",
      "================================================================================\n",
      "FILES SAVED\n",
      "================================================================================\n",
      "✓ meta_scan_csvs/train_metadata.csv\n",
      "✓ meta_scan_csvs/test_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the inventory we just created\n",
    "metadata_df = pd.read_csv(\"meta_scan_csvs/data_inventory.csv\")\n",
    "\n",
    "# Create a stratification column combining room_type and scan_type\n",
    "metadata_df['strata'] = metadata_df['room_type'] + '_' + metadata_df['scan_type']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MANUAL STRATIFIED TRAIN/TEST SPLIT (~80/20)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Manually assign splits to ensure good distribution\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for strata in metadata_df['strata'].unique():\n",
    "    strata_df = metadata_df[metadata_df['strata'] == strata]\n",
    "    n = len(strata_df)\n",
    "    \n",
    "    # Calculate split\n",
    "    n_test = max(1, round(n * 0.2))  # At least 1 for test\n",
    "    n_train = n - n_test\n",
    "    \n",
    "    print(f\"{strata}: {n} total → {n_train} train, {n_test} test\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    shuffled = strata_df.sample(frac=1, random_state=42)\n",
    "    test_indices.extend(shuffled.index[:n_test].tolist())\n",
    "    train_indices.extend(shuffled.index[n_test:].tolist())\n",
    "\n",
    "train_df = metadata_df.loc[train_indices]\n",
    "test_df = metadata_df.loc[test_indices]\n",
    "\n",
    "print(f\"\\nTotal files: {len(metadata_df)}\")\n",
    "print(f\"Training files: {len(train_df)} ({len(train_df)/len(metadata_df)*100:.1f}%)\")\n",
    "print(f\"Testing files: {len(test_df)} ({len(test_df)/len(metadata_df)*100:.1f}%)\")\n",
    "\n",
    "# Save the splits\n",
    "train_df.to_csv(\"meta_scan_csvs/train_metadata.csv\", index=False)\n",
    "test_df.to_csv(\"meta_scan_csvs/test_metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ meta_scan_csvs/train_metadata.csv\")\n",
    "print(\"✓ meta_scan_csvs/test_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7998a8c-4fa1-4c56-b977-9a6d56396d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING TRAIN SET (COMMON FEATURES ONLY)\n",
      "================================================================================\n",
      "\n",
      "blinds_9_cleaned.csv: 290 rows → 5 windows (54 features)\n",
      "blinds_7_cleaned.csv: 301 rows → 6 windows (54 features)\n",
      "blinds_10_cleaned.csv: 1719 rows → 34 windows (54 features)\n",
      "blinds_8_cleaned.csv: 312 rows → 6 windows (54 features)\n",
      "blinds_motion_5_cleaned.csv: 327 rows → 6 windows (54 features)\n",
      "blinds_motion_3_cleaned.csv: 335 rows → 6 windows (54 features)\n",
      "blinds_motion_1_cleaned.csv: 1838 rows → 36 windows (54 features)\n",
      "blinds_motion_4_cleaned.csv: 312 rows → 6 windows (54 features)\n",
      "blinds_object_5_cleaned.csv: 1827 rows → 36 windows (54 features)\n",
      "blinds_object_3_cleaned.csv: 327 rows → 6 windows (54 features)\n",
      "blinds_object_1_cleaned.csv: 1821 rows → 36 windows (54 features)\n",
      "blinds_object_4_cleaned.csv: 325 rows → 6 windows (54 features)\n",
      "blinds_person_5_cleaned.csv: 327 rows → 6 windows (54 features)\n",
      "blinds_person_3_cleaned.csv: 326 rows → 6 windows (54 features)\n",
      "blinds_person_1_cleaned.csv: 350 rows → 7 windows (54 features)\n",
      "blinds_person_4_cleaned.csv: 312 rows → 6 windows (54 features)\n",
      "blinds_up_9_cleaned.csv: 310 rows → 6 windows (54 features)\n",
      "blinds_up_7_cleaned.csv: 1711 rows → 34 windows (54 features)\n",
      "blinds_up_10_cleaned.csv: 319 rows → 6 windows (54 features)\n",
      "blinds_up_8_cleaned.csv: 305 rows → 6 windows (54 features)\n",
      "blinds_up_motion_9_cleaned.csv: 295 rows → 5 windows (54 features)\n",
      "blinds_up_motion_7_cleaned.csv: 1697 rows → 33 windows (54 features)\n",
      "blinds_up_motion_10_cleaned.csv: 311 rows → 6 windows (54 features)\n",
      "blinds_up_motion_8_cleaned.csv: 371 rows → 7 windows (54 features)\n",
      "blinds_up_object_5_cleaned.csv: 1836 rows → 36 windows (54 features)\n",
      "blinds_up_object_3_cleaned.csv: 1891 rows → 37 windows (54 features)\n",
      "blinds_up_object_1_cleaned.csv: 1755 rows → 35 windows (54 features)\n",
      "blinds_up_object_4_cleaned.csv: 313 rows → 6 windows (54 features)\n",
      "blinds_up_person_9_cleaned.csv: 304 rows → 6 windows (54 features)\n",
      "blinds_up_person_7_cleaned.csv: 319 rows → 6 windows (54 features)\n",
      "blinds_up_person_10_cleaned.csv: 318 rows → 6 windows (54 features)\n",
      "blinds_up_person_8_cleaned.csv: 1851 rows → 37 windows (54 features)\n",
      "hallway_5_cleaned.csv: 1493 rows → 29 windows (54 features)\n",
      "hallway_3_cleaned.csv: 306 rows → 6 windows (54 features)\n",
      "hallway_1_cleaned.csv: 300 rows → 6 windows (54 features)\n",
      "hallway_4_cleaned.csv: 320 rows → 6 windows (54 features)\n",
      "hallway_motion_5_cleaned.csv: 314 rows → 6 windows (54 features)\n",
      "hallway_motion_3_cleaned.csv: 313 rows → 6 windows (54 features)\n",
      "hallway_motion_1_cleaned.csv: 329 rows → 6 windows (54 features)\n",
      "hallway_motion_4_cleaned.csv: 1869 rows → 37 windows (54 features)\n",
      "hallway_object_5_cleaned.csv: 306 rows → 6 windows (54 features)\n",
      "hallway_object_3_cleaned.csv: 1739 rows → 34 windows (54 features)\n",
      "hallway_object_1_cleaned.csv: 2024 rows → 40 windows (54 features)\n",
      "hallway_object_4_cleaned.csv: 304 rows → 6 windows (54 features)\n",
      "hallway_person_5_cleaned.csv: 1799 rows → 35 windows (54 features)\n",
      "hallway_person_3_cleaned.csv: 345 rows → 6 windows (54 features)\n",
      "hallway_person_1_cleaned.csv: 340 rows → 6 windows (54 features)\n",
      "hallway_person_4_cleaned.csv: 1977 rows → 39 windows (54 features)\n",
      "kitchen_1_cleaned.csv: 320 rows → 6 windows (54 features)\n",
      "kitchen_6_cleaned.csv: 250 rows → 5 windows (54 features)\n",
      "kitchen_2_cleaned.csv: 296 rows → 5 windows (54 features)\n",
      "kitchen_4_cleaned.csv: 1668 rows → 33 windows (54 features)\n",
      "kitchen_3_cleaned.csv: 312 rows → 6 windows (54 features)\n",
      "kitchen_9_cleaned.csv: 1971 rows → 39 windows (54 features)\n",
      "kitchen_motion_5_cleaned.csv: 342 rows → 6 windows (54 features)\n",
      "kitchen_motion_3_cleaned.csv: 1837 rows → 36 windows (54 features)\n",
      "kitchen_motion_1_cleaned.csv: 313 rows → 6 windows (54 features)\n",
      "kitchen_motion_4_cleaned.csv: 338 rows → 6 windows (54 features)\n",
      "kitchen_object_5_cleaned.csv: 340 rows → 6 windows (54 features)\n",
      "kitchen_object_3_cleaned.csv: 1839 rows → 36 windows (54 features)\n",
      "kitchen_object_1_cleaned.csv: 1748 rows → 34 windows (54 features)\n",
      "kitchen_object_4_cleaned.csv: 1839 rows → 36 windows (54 features)\n",
      "kitchen_person_5_cleaned.csv: 322 rows → 6 windows (54 features)\n",
      "kitchen_person_2_cleaned.csv: 335 rows → 6 windows (54 features)\n",
      "kitchen_person_4_cleaned.csv: 308 rows → 6 windows (54 features)\n",
      "lab_5_cleaned.csv: 1717 rows → 34 windows (54 features)\n",
      "lab_3_cleaned.csv: 312 rows → 6 windows (54 features)\n",
      "lab_1_cleaned.csv: 1749 rows → 34 windows (54 features)\n",
      "lab_4_cleaned.csv: 312 rows → 6 windows (54 features)\n",
      "lab_motion_5_cleaned.csv: 1859 rows → 37 windows (54 features)\n",
      "lab_motion_3_cleaned.csv: 330 rows → 6 windows (54 features)\n",
      "lab_motion_1_cleaned.csv: 300 rows → 6 windows (54 features)\n",
      "lab_motion_4_cleaned.csv: 322 rows → 6 windows (54 features)\n",
      "lab_object_5_cleaned.csv: 296 rows → 5 windows (54 features)\n",
      "lab_object_3_cleaned.csv: 313 rows → 6 windows (54 features)\n",
      "lab_object_1_cleaned.csv: 1850 rows → 37 windows (54 features)\n",
      "lab_object_4_cleaned.csv: 297 rows → 5 windows (54 features)\n",
      "lab_person_5_cleaned.csv: 309 rows → 6 windows (54 features)\n",
      "lab_person_1_cleaned.csv: 341 rows → 6 windows (54 features)\n",
      "lab_person_3_cleaned.csv: 328 rows → 6 windows (54 features)\n",
      "\n",
      "================================================================================\n",
      "TRAIN SUMMARY\n",
      "================================================================================\n",
      "Total CSVs processed: 80\n",
      "Total windows created: 1244\n",
      "Windows per CSV (avg): 15.6\n",
      "Features per window: 54 (common features only)\n",
      "\n",
      "✓ Saved to: meta_scan_csvs\\windowed_data\\train_windows_common.pkl\n",
      "\n",
      "Windows by room type:\n",
      "room_type\n",
      "blinds       214\n",
      "blinds_up    272\n",
      "hallway      274\n",
      "kitchen      278\n",
      "lab          206\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "PROCESSING TEST SET (COMMON FEATURES ONLY)\n",
      "================================================================================\n",
      "\n",
      "blinds_6_cleaned.csv: 1365 rows → 27 windows (54 features)\n",
      "blinds_motion_2_cleaned.csv: 329 rows → 6 windows (54 features)\n",
      "blinds_object_2_cleaned.csv: 327 rows → 6 windows (54 features)\n",
      "blinds_person_2_cleaned.csv: 313 rows → 6 windows (54 features)\n",
      "blinds_up_6_cleaned.csv: 295 rows → 5 windows (54 features)\n",
      "blinds_up_motion_6_cleaned.csv: 298 rows → 5 windows (54 features)\n",
      "blinds_up_object_2_cleaned.csv: 1765 rows → 35 windows (54 features)\n",
      "blinds_up_person_6_cleaned.csv: 309 rows → 6 windows (54 features)\n",
      "hallway_2_cleaned.csv: 305 rows → 6 windows (54 features)\n",
      "hallway_motion_2_cleaned.csv: 327 rows → 6 windows (54 features)\n",
      "hallway_object_2_cleaned.csv: 1712 rows → 34 windows (54 features)\n",
      "hallway_person_2_cleaned.csv: 1820 rows → 36 windows (54 features)\n",
      "kitchen_10_cleaned.csv: 2064 rows → 41 windows (54 features)\n",
      "kitchen_motion_2_cleaned.csv: 1647 rows → 32 windows (54 features)\n",
      "kitchen_object_2_cleaned.csv: 326 rows → 6 windows (54 features)\n",
      "kitchen_person_3_cleaned.csv: 1754 rows → 35 windows (54 features)\n",
      "lab_2_cleaned.csv: 319 rows → 6 windows (54 features)\n",
      "lab_motion_2_cleaned.csv: 317 rows → 6 windows (54 features)\n",
      "lab_object_2_cleaned.csv: 1824 rows → 36 windows (54 features)\n",
      "lab_person_2_cleaned.csv: 1955 rows → 39 windows (54 features)\n",
      "\n",
      "================================================================================\n",
      "TEST SUMMARY\n",
      "================================================================================\n",
      "Total CSVs processed: 20\n",
      "Total windows created: 379\n",
      "Windows per CSV (avg): 18.9\n",
      "Features per window: 54 (common features only)\n",
      "\n",
      "✓ Saved to: meta_scan_csvs\\windowed_data\\test_windows_common.pkl\n",
      "\n",
      "Windows by room type:\n",
      "room_type\n",
      "blinds        45\n",
      "blinds_up     51\n",
      "hallway       82\n",
      "kitchen      114\n",
      "lab           87\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "WINDOWING COMPLETE (COMMON FEATURES)\n",
      "================================================================================\n",
      "Training windows: 1244\n",
      "Testing windows: 379\n",
      "Window size: 50 timesteps\n",
      "Features per window: 54\n",
      "No padding needed - all windows have same shape!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "COMMON_COLUMNS = [\n",
    "    '% Prims Clipped',\n",
    "    '% Prims Trivially Rejected',\n",
    "    '% Stalled on System Memory',\n",
    "    '% Texture L2 Miss',\n",
    "    '% Vertex Fetch Stall',\n",
    "    'ALU / Fragment',\n",
    "    'ALU / Vertex',\n",
    "    'Average Polygon Area',\n",
    "    'Average Vertices / Polygon',\n",
    "    'Avg Bytes / Fragment',\n",
    "    'Avg Bytes / Vertex',\n",
    "    'Avg Preemption Delay',\n",
    "    'Clocks / Second',\n",
    "    'EFU / Fragment',\n",
    "    'EFU / Vertex',\n",
    "    'Fragment ALU Instructions / Sec (Full)',\n",
    "    'Fragment ALU Instructions / Sec (Half)',\n",
    "    'Fragment EFU Instructions / Second',\n",
    "    'Fragment Instructions / Second',\n",
    "    'Fragments Shaded / Second',\n",
    "    'GPU % Bus Busy',\n",
    "    'GPU Frequency',\n",
    "    'L1 Texture Cache Miss Per Pixel',\n",
    "    'Pre-clipped Polygons/Second',\n",
    "    'Preemptions / second',\n",
    "    'Read Total (Bytes/sec)',\n",
    "    'Reused Vertices / Second',\n",
    "    'SP Memory Read (Bytes/Second)',\n",
    "    'Texture Memory Read BW (Bytes/Second)',\n",
    "    'Textures / Fragment',\n",
    "    'Textures / Vertex',\n",
    "    'Vertex Instructions / Second',\n",
    "    'Vertex Memory Read (Bytes/Second)',\n",
    "    'Vertices Shaded / Second',\n",
    "    'Write Total (Bytes/sec)',\n",
    "    'app_rss_mb',\n",
    "    'app_uss_mb',\n",
    "    'app_vss_mb',\n",
    "    'available_memory_mb',\n",
    "    'cpu_frequency_mhz',\n",
    "    'cpu_level',\n",
    "    'cpu_util_0',\n",
    "    'cpu_util_1',\n",
    "    'cpu_util_2',\n",
    "    'cpu_util_3',\n",
    "    'cpu_util_4',\n",
    "    'cpu_util_5',\n",
    "    'display_refresh_rate',\n",
    "    'gpu_frequency_mhz',\n",
    "    'gpu_level',\n",
    "    'gpu_util',\n",
    "    'mem_frequency_mhz',\n",
    "    'timewarp_gpu_ms',\n",
    "]\n",
    "\n",
    "def create_windows(df, window_size=50):  # Changed from 75 to 50\n",
    "    \"\"\"\n",
    "    Create non-overlapping tumbling windows from a time series dataframe.\n",
    "    Returns a list of window dataframes.\n",
    "    \"\"\"\n",
    "    num_windows = len(df) // window_size\n",
    "    windows = []\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        start_idx = i * window_size\n",
    "        end_idx = start_idx + window_size\n",
    "        window = df.iloc[start_idx:end_idx].copy()\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "def process_dataset(metadata_df, output_dir, dataset_name):\n",
    "    \"\"\"\n",
    "    Process all CSVs in a dataset (train or test), create windows using ONLY common features.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_windows_data = []\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PROCESSING {dataset_name.upper()} SET (COMMON FEATURES ONLY)\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        csv_path = Path(row['filepath'])\n",
    "        room_type = row['room_type']\n",
    "        scan_type = row['scan_type']\n",
    "        trial_number = row['trial_number']\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Select ONLY common columns\n",
    "        available_common = [col for col in COMMON_COLUMNS if col in df.columns]\n",
    "        \n",
    "        if len(available_common) != len(COMMON_COLUMNS):\n",
    "            missing = set(COMMON_COLUMNS) - set(available_common)\n",
    "            print(f\"WARNING: {row['filename']} missing columns: {missing}\")\n",
    "        \n",
    "        df = df[available_common]\n",
    "        \n",
    "        # Create windows with size 50\n",
    "        windows = create_windows(df, window_size=50)  # Changed from 75 to 50\n",
    "        \n",
    "        print(f\"{row['filename']}: {len(df)} rows → {len(windows)} windows (54 features)\")\n",
    "        \n",
    "        # Save each window with metadata\n",
    "        for window_idx, window_df in enumerate(windows):\n",
    "            window_data = {\n",
    "                'original_filename': row['filename'],\n",
    "                'room_type': room_type,\n",
    "                'scan_type': scan_type,\n",
    "                'trial_number': trial_number,\n",
    "                'window_id': window_idx,\n",
    "                'window_data': window_df.values  # Store as numpy array\n",
    "            }\n",
    "            all_windows_data.append(window_data)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{dataset_name.upper()} SUMMARY\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Total CSVs processed: {len(metadata_df)}\")\n",
    "    print(f\"Total windows created: {len(all_windows_data)}\")\n",
    "    print(f\"Windows per CSV (avg): {len(all_windows_data) / len(metadata_df):.1f}\")\n",
    "    print(f\"Features per window: 54 (common features only)\")\n",
    "    \n",
    "    # Save as pickle for easy loading later\n",
    "    windows_df = pd.DataFrame(all_windows_data)\n",
    "    output_file = output_path / f\"{dataset_name}_windows_common.pkl\"\n",
    "    windows_df.to_pickle(output_file)\n",
    "    print(f\"\\n✓ Saved to: {output_file}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"\\nWindows by room type:\")\n",
    "    print(windows_df.groupby('room_type').size())\n",
    "    \n",
    "    return windows_df\n",
    "\n",
    "# Load train and test metadata\n",
    "train_metadata = pd.read_csv(\"meta_scan_csvs/train_metadata.csv\")\n",
    "test_metadata = pd.read_csv(\"meta_scan_csvs/test_metadata.csv\")\n",
    "\n",
    "# Process training set\n",
    "train_windows = process_dataset(\n",
    "    train_metadata, \n",
    "    \"meta_scan_csvs/windowed_data\", \n",
    "    \"train\"\n",
    ")\n",
    "\n",
    "# Process testing set\n",
    "test_windows = process_dataset(\n",
    "    test_metadata, \n",
    "    \"meta_scan_csvs/windowed_data\", \n",
    "    \"test\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"WINDOWING COMPLETE (COMMON FEATURES)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Training windows: {len(train_windows)}\")\n",
    "print(f\"Testing windows: {len(test_windows)}\")\n",
    "print(f\"Window size: 50 timesteps\")\n",
    "print(f\"Features per window: 54\")\n",
    "print(f\"No padding needed - all windows have same shape!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba4788b-8e1e-423d-b767-c704239ab17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
